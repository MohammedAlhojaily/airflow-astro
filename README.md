# ğŸš¦ ETL Toll Data Pipeline using Apache Airflow

This is an ETL (Extract, Transform, Load) pipeline built using **Apache Airflow** and **Python**. It processes toll data from multiple file formats and transforms it into a single consolidated CSV file. This project demonstrates task orchestration, file handling, and basic data transformation using Airflow and Pandas.

---

## ğŸ§© Project Overview

The ETL pipeline performs the following steps:

1. **Download** a `.tgz` archive containing toll data.
2. **Extract** the archive into a local staging folder.
3. **Process** and extract data from:
   - `vehicle-data.csv`
   - `tollplaza-data.tsv`
   - `payment-data.txt` (fixed-width format)
4. **Consolidate** data into a single DataFrame.
5. **Transform** vehicle types to uppercase and save the final result as `transformed_data.csv`.

All tasks are managed using Airflowâ€™s modern `@dag` and `@task` decorators, and the data is handled using `pandas`.

---

## ğŸ“ Directory Structure

project-root/
â”‚
â”œâ”€â”€ dags/
â”‚ â”œâ”€â”€ etl_toll_pipeline.py # Main Airflow DAG
â”‚ â””â”€â”€ staging/ # Staging folder for all data files
â”‚ â”œâ”€â”€ vehicle-data.csv
â”‚ â”œâ”€â”€ tollplaza-data.tsv
â”‚ â”œâ”€â”€ payment-data.txt
â”‚ â”œâ”€â”€ csv_data.csv
â”‚ â”œâ”€â”€ tsv_data.csv
â”‚ â”œâ”€â”€ fixed_width_data.csv
â”‚ â”œâ”€â”€ extracted_data.csv
â”‚ â””â”€â”€ transformed_data.csv
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸš€ Setup Instructions

### 1. Install Apache Airflow

Follow the official [Airflow Installation Guide](https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html)

Example using `pip`:

```bash
pip install apache-airflow==2.9.1 \
  --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.9.1/constraints-3.8.txt"

Clone This Repository
git clone https://github.com/your-username/etl-toll-data-airflow.git
cd etl-toll-data-airflow

Copy DAG to Airflow
# Linux/Mac
cp dags/etl_toll_pipeline.py ~/airflow/dags/

# Windows (default Airflow Home for Astronomer or custom setups)
copy dags\etl_toll_pipeline.py C:\airflow-astro\dags\

Start Airflow
airflow db init
airflow scheduler
airflow webserver --port 8080

Open Airflow UI at: http://localhost:8080
Enable the DAG named ETL_toll_data.

Trigger the DAG
Trigger the DAG manually from the UI or wait for its daily schedule.

ğŸ“¦ Dependencies
Install required Python libraries using:
pip install -r requirements.txt
